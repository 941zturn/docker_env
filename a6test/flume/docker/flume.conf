#
# Setup some names
#
agent.sources  = sr-kafka
agent.channels = ch-kafka
agent.sinks    = sk-hdfs

#
# Configure same Kafka source for all channels
#
agent.sources.sr-kafka.channels = ch-kafka
agent.sources.sr-kafka.type = org.apache.flume.source.kafka.KafkaSource
agent.sources.sr-kafka.kafka.bootstrap.servers = kafka1:9092
# agent.sources.sr-kafka.kafka.consumer.group.id = flume_source
agent.sources.sr-kafka.kafka.topics = wzh_filebeat
# Grabs in batches of 500 or every second
agent.sources.sr-kafka.batchSize = 1
agent.sources.sr-kafka.batchDurationMillis = 1000
# Read from start of topic
agent.sources.sr-kafka.kafka.consumer.auto.offset.reset = earliest


#
# Configure interceptors
#
agent.sources.sr-kafka.interceptors = attach-schema in-morphline-etl

agent.sources.sr-kafka.interceptors.attach-schema.type = static
agent.sources.sr-kafka.interceptors.attach-schema.key = flume.avro.schema.url
agent.sources.sr-kafka.interceptors.attach-schema.value = file://opt/schema/schema.avsc


agent.sources.sr-kafka.interceptors.in-morphline-etl.type = org.apache.flume.sink.solr.morphline.MorphlineInterceptor$Builder
agent.sources.sr-kafka.interceptors.in-morphline-etl.morphlineFile = /opt/flume-config/morphline.conf
agent.sources.sr-kafka.interceptors.in-morphline-etl.morphlineId = morphline_id




#
# Configure the channels we multiplexed into
#
agent.channels.ch-kafka.type = org.apache.flume.channel.kafka.KafkaChannel
# agent.channels.ch-kafka.brokerList = HOST1:PORT,HOST2:PORT,HOST3:PORT
agent.channels.ch-kafka.kafka.bootstrap.servers = kafka1:9092
# agent.channels.ch-kafka.kafka.consumer.group.id = flume_channel
agent.channels.ch-kafka.kafka.topic = flume-channel
agent.channels.ch-kafka.kafka.consumer.auto.offset.reset = earliest



#
# Configure sinks; We pull from Kafka in batches and write large files into HDFS.
#
agent.sinks.sk-hdfs.channel = ch-kafka
agent.sinks.sk-hdfs.type = hdfs
agent.sinks.sk-hdfs.hdfs.path = hdfs://namenode:9000/flume/log00/%Y%m%d%H
# Prefix files with the Flume agent&#039;s hostname so we can run multiple agents without collision
# agent.sinks.sk-hdfs.hdfs.filePrefix = %{flume_host}
# agent.sinks.sk-hdfs.hdfs.filePrefix = log00_
# agent.sinks.sk-hdfs.hdfs.inUsePrefix = _
# Hive needs files to end in .avro
# agent.sinks.sk-hdfs.hdfs.fileSuffix = .avro
# Roll files in HDFS every 5 min or at 255MB; don&#039;t roll based on number of records
# We roll at 255MB because our block size is 128MB, we want 2 full blocks without going over
agent.sinks.sk-hdfs.hdfs.rollInterval = 300
agent.sinks.sk-hdfs.hdfs.rollSize = 267386880
agent.sinks.sk-hdfs.hdfs.rollCount = 0
# Write to HDFS file in batches of 500 records
agent.sinks.sk-hdfs.hdfs.batchSize = 1
# We already serialized and encoded the record into Avro in Morphline so just write the byte array
agent.sinks.sk-hdfs.hdfs.fileType = DataStream
# Give us a higher timeout because we are writing in batch
agent.sinks.sk-hdfs.hdfs.callTimeout = 60000
# Use current time in UTC for the value of `record_ymdh=%Y%m%d%H` above
agent.sinks.sk-hdfs.hdfs.timeZone = CST
agent.sinks.sk-hdfs.hdfs.useLocalTimeStamp = true
# Our record is serialized via Avro
# agent.sinks.sk-hdfs.serializer = org.apache.flume.sink.hdfs.AvroEventSerializer$Builder
