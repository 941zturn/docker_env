#
# Setup some names
#
agent.sources  = sr-kafka
agent.channels = ch-kafka
agent.sinks    = sk-hdfs

#
# Configure same Kafka source for all channels
#
agent.sources.sr-kafka.channels = ch-kafka
agent.sources.sr-kafka.type = org.apache.flume.source.kafka.KafkaSource
agent.sources.sr-kafka.kafka.bootstrap.servers = 11.11.157.135:9092
agent.sources.sr-kafka.kafka.consumer.group.id = flume_source_01
agent.sources.sr-kafka.kafka.topics = datalake
# Grabs in batches of 500 or every second
agent.sources.sr-kafka.batchSize = 100
agent.sources.sr-kafka.batchDurationMillis = 1000
# Read from start of topic
agent.sources.sr-kafka.kafka.consumer.auto.offset.reset = earliest


#
# Configure interceptors
#


#
# Configure the channels we multiplexed into
#
# agent.channels.ch-kafka.type = org.apache.flume.channel.kafka.KafkaChannel
# agent.channels.ch-kafka.brokerList = HOST1:PORT,HOST2:PORT,HOST3:PORT
# agent.channels.ch-kafka.kafka.bootstrap.servers = 11.11.157.135:9092
# agent.channels.ch-kafka.kafka.consumer.group.id = flume_channel
# agent.channels.ch-kafka.kafka.topic = flume-channel
# agent.channels.ch-kafka.kafka.consumer.auto.offset.reset = earliest

agent.channels.ch-kafka.type = file
agent.channels.ch-kafka.checkpointDir = /data/flume/checkpoint
agent.channels.ch-kafka.kafka.dataDirs = /data/flume/data



#
# Configure sinks; We pull from Kafka in batches and write large files into HDFS.
#
agent.sinks.sk-hdfs.channel = ch-kafka
agent.sinks.sk-hdfs.type = hdfs
agent.sinks.sk-hdfs.hdfs.path = hdfs://11.11.157.145:9000/flume/a6/%Y%m%d
# Prefix files with the Flume agent&#039;s hostname so we can run multiple agents without collision
# agent.sinks.sk-hdfs.hdfs.filePrefix = %{flume_host}
agent.sinks.sk-hdfs.hdfs.filePrefix = a6_flume
agent.sinks.sk-hdfs.hdfs.inUsePrefix = _
# Hive needs files to end in .avro
agent.sinks.sk-hdfs.hdfs.fileSuffix = .json
# Roll files in HDFS every 5 min or at 255MB; don&#039;t roll based on number of records
# We roll at 255MB because our block size is 128MB, we want 2 full blocks without going over
agent.sinks.sk-hdfs.hdfs.rollInterval = 600
agent.sinks.sk-hdfs.hdfs.rollSize = 267386880
agent.sinks.sk-hdfs.hdfs.rollCount = 0
# Write to HDFS file in batches of 500 records
agent.sinks.sk-hdfs.hdfs.batchSize = 100
# We already serialized and encoded the record into Avro in Morphline so just write the byte array
agent.sinks.sk-hdfs.hdfs.fileType = DataStream
# Give us a higher timeout because we are writing in batch
agent.sinks.sk-hdfs.hdfs.callTimeout = 60000
# Use current time in UTC for the value of `record_ymdh=%Y%m%d%H` above
agent.sinks.sk-hdfs.hdfs.timeZone = CST
agent.sinks.sk-hdfs.hdfs.useLocalTimeStamp = true
# Our record is serialized via Avro
agent.sinks.sk-hdfs.serializer = text
# agent.sinks.sk-hdfs.serializer.schemaURL = file://opt/schema/schema.avsc
# agent.sinks.sk-hdfs.serializer.schemaURL = hdfs://namenode:9000/flume/.schema/schema.avsc
