#
# Setup some names
#
agent.sources  = sr-kafka
agent.channels = ch-kafka
agent.sinks    = sk-hdfs

#
# Configure same Kafka source for all channels
#
agent.sources.sr-kafka.channels = ch-kafka
agent.sources.sr-kafka.type = org.apache.flume.source.kafka.KafkaSource
agent.sources.sr-kafka.kafka.bootstrap.servers = 11.11.157.135:9092
# agent.sources.sr-kafka.kafka.consumer.group.id = flume_source
agent.sources.sr-kafka.kafka.topics = datalake
# Grabs in batches of 500 or every second
agent.sources.sr-kafka.batchSize = 500
agent.sources.sr-kafka.batchDurationMillis = 1000
# Read from start of topic
agent.sources.sr-kafka.kafka.consumer.auto.offset.reset = earliest


#
# Configure interceptors
#
agent.sources.sr-kafka.interceptors = attach-schema in-morphline-etl

agent.sources.sr-kafka.interceptors.attach-schema.type = static
agent.sources.sr-kafka.interceptors.attach-schema.key = flume.avro.schema.url
agent.sources.sr-kafka.interceptors.attach-schema.value = hdfs://11.11.157.192:9000/flume/.schema/a6.schema.avsc


agent.sources.sr-kafka.interceptors.in-morphline-etl.type = org.apache.flume.sink.solr.morphline.MorphlineInterceptor$Builder
agent.sources.sr-kafka.interceptors.in-morphline-etl.morphlineFile = /opt/flume-config/morphline.conf
agent.sources.sr-kafka.interceptors.in-morphline-etl.morphlineId = morphline_id




#
# Configure the channels we multiplexed into
#
agent.channels.ch-kafka.type = file
agent.channels.ch-kafka.checkpointDir = /mnt/flume/checkpoint
agent.channels.ch-kafka.kafka.dataDirs = /mnt/flume/data




#
# Configure sinks; We pull from Kafka in batches and write large files into HDFS.
#
agent.sinks.sk-hdfs.channel = ch-kafka
agent.sinks.sk-hdfs.type = hdfs
agent.sinks.sk-hdfs.hdfs.path = hdfs://11.11.157.192:9000/flume/a6/%Y%m%d
# Prefix files with the Flume agent&#039;s hostname so we can run multiple agents without collision
# agent.sinks.sk-hdfs.hdfs.filePrefix = %{flume_host}
agent.sinks.sk-hdfs.hdfs.filePrefix = a6_
agent.sinks.sk-hdfs.hdfs.inUsePrefix = _
# Hive needs files to end in .avro
agent.sinks.sk-hdfs.hdfs.fileSuffix = .avro
# Roll files in HDFS every 5 min or at 255MB; don&#039;t roll based on number of records
# We roll at 255MB because our block size is 128MB, we want 2 full blocks without going over
agent.sinks.sk-hdfs.hdfs.rollInterval = 300
agent.sinks.sk-hdfs.hdfs.rollSize = 267386880
agent.sinks.sk-hdfs.hdfs.rollCount = 0
# Write to HDFS file in batches of 500 records
agent.sinks.sk-hdfs.hdfs.batchSize = 500
# We already serialized and encoded the record into Avro in Morphline so just write the byte array
agent.sinks.sk-hdfs.hdfs.fileType = DataStream
# Give us a higher timeout because we are writing in batch
agent.sinks.sk-hdfs.hdfs.callTimeout = 60000
# Use current time in UTC for the value of `record_ymdh=%Y%m%d%H` above
agent.sinks.sk-hdfs.hdfs.timeZone = CST
agent.sinks.sk-hdfs.hdfs.useLocalTimeStamp = true
# Our record is serialized via Avro
agent.sinks.sk-hdfs.serializer = org.apache.flume.sink.hdfs.AvroEventSerializer$Builder
# agent.sinks.sk-hdfs.serializer.schemaURL = file://opt/schema/schema.avsc
agent.sinks.sk-hdfs.serializer.schemaURL = hdfs://11.11.157.192:9000/flume/.schema/a6.schema.avsc
