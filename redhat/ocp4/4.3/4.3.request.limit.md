# ocp 4.3 request / limit

展现 k8s/openshift 如何处理 cpu, memory 的 request/limit

video
- https://youtu.be/s3G6VN4nUZE
- https://www.bilibili.com/video/BV1GZ4y1H7VS/

## cpu

```bash
# 我们的测试主机是24C/256G mem的
# 创建一个pod，占用20C
cat << EOF > demo.yaml
---
kind: Pod
apiVersion: v1
metadata:
  name: demo-pod1
spec:
  nodeSelector:
    kubernetes.io/hostname: 'infra1.hsc.redhat.ren'
  restartPolicy: Always
  containers:
    - name: demo1
      image: >- 
        registry.redhat.ren:5443/docker.io/wangzheng422/centos:centos7-test
      env:
        - name: key
          value: value
      command: ["iperf3", "-s", "-p" ]
      args: [ "6666" ]
      imagePullPolicy: Always
      resources:
        requests:
          cpu: 10.0
          memory: 8Gi
        limits:
          cpu: 60.0
          memory: 800Gi
EOF
oc apply -n demo -f demo.yaml

# 再创建一个pod，占用20C就不成功了
cat << EOF > demo1.yaml
---
kind: Pod
apiVersion: v1
metadata:
  name: demo-pod2
spec:
  nodeSelector:
    kubernetes.io/hostname: 'infra1.hsc.redhat.ren'
  restartPolicy: Always
  containers:
    - name: demo1
      image: >- 
        registry.redhat.ren:5443/docker.io/wangzheng422/centos:centos7-test
      env:
        - name: key
          value: value
      command: ["iperf3", "-s", "-p" ]
      args: [ "6666" ]
      imagePullPolicy: Always
      resources:
        requests:
          cpu: 20.0
          memory: 8Gi
        limits:
          cpu: 60.0
          memory: 800Gi

EOF
oc apply -n demo -f demo1.yaml

oc get pod -o wide
# NAME        READY   STATUS    RESTARTS   AGE   IP            NODE                    NOMINATED NODE   READINESS GATES
# demo-pod1   1/1     Running   0          12m   10.254.5.42   infra1.hsc.redhat.ren   <none>           <none>
# demo-pod2   0/1     Pending   0          45s   <none>        <none>                  <none>           <none>

oc describe pod/demo-pod2
# Events:
#   Type     Reason            Age        From               Message
#   ----     ------            ----       ----               -------
#   Warning  FailedScheduling  <unknown>  default-scheduler  0/8 nodes are available: 1 Insufficient memory, 7 Insufficient cpu, 7 node(s) didn't match node selector.

oc delete -n demo -f demo1.yaml

# 创建一个pod，request是1C
cat << EOF > demo1.yaml
---
kind: Pod
apiVersion: v1
metadata:
  name: demo-pod2
spec:
  nodeSelector:
    kubernetes.io/hostname: 'infra1.hsc.redhat.ren'
  restartPolicy: Always
  containers:
    - name: demo1
      image: >- 
        registry.redhat.ren:5443/docker.io/wangzheng422/centos:centos7-test
      env:
        - name: key
          value: value
      command: ["iperf3", "-s", "-p" ]
      args: [ "6666" ]
      imagePullPolicy: Always
      resources:
        requests:
          cpu: 1.0
          memory: 8Gi
        limits:
          cpu: 60.0
          memory: 800Gi

EOF
oc apply -n demo -f demo1.yaml

# 在容器内部施加压力
oc exec -t demo-pod1 -- sysbench cpu --report-interval=3 --threads=60 --time=999999999 run
oc exec -t demo-pod2 -- sysbench cpu --report-interval=3 --threads=60 --time=999999999 run

# 在infra1上用htop查看进程，可以看到cpu比率为10:1


# restore
oc delete -n demo -f demo.yaml
oc delete -n demo -f demo1.yaml

```
![](imgs/2020-06-03-21-05-03.png)

# memory


```bash
# 创建2个测试pod
cat << EOF > demo.yaml
---
kind: Pod
apiVersion: v1
metadata:
  name: demo-pod1
spec:
  nodeSelector:
    kubernetes.io/hostname: 'infra1.hsc.redhat.ren'
  restartPolicy: Always
  containers:
    - name: demo1
      image: >- 
        registry.redhat.ren:5443/docker.io/wangzheng422/centos:centos7-test
      env:
        - name: key
          value: value
      command: ["iperf3", "-s", "-p" ]
      args: [ "6666" ]
      imagePullPolicy: Always
      resources:
        requests:
          cpu: 1.0
          memory: 8Gi
        limits:
          cpu: 60.0
          memory: 100Gi
---
kind: Pod
apiVersion: v1
metadata:
  name: demo-pod2
spec:
  nodeSelector:
    kubernetes.io/hostname: 'infra1.hsc.redhat.ren'
  restartPolicy: Always
  containers:
    - name: demo1
      image: >- 
        registry.redhat.ren:5443/docker.io/wangzheng422/centos:centos7-test
      env:
        - name: key
          value: value
      command: ["iperf3", "-s", "-p" ]
      args: [ "6666" ]
      imagePullPolicy: Always
      resources:
        requests:
          cpu: 1.0
          memory: 8Gi
        limits:
          cpu: 60.0
          memory: 100Gi

EOF
oc apply -n demo -f demo.yaml

oc get pod
# NAME        READY   STATUS    RESTARTS   AGE
# demo-pod1   1/1     Running   0          18s
# demo-pod2   1/1     Running   0          18s

# 在容器内部施加压力
oc exec -t demo-pod1 -- stress-ng --vm 1 --vm-bytes 150G --oomable --metrics -v -t 9y

# 在infra1上用htop查看进程，发现到达100G后，kernel杀掉了进程

# infra上 dmesg
# [856808.927985] stress-ng-vm invoked oom-killer: gfp_mask=0xd0, order=0, oom_score_adj=1000
# [856808.927991] stress-ng-vm cpuset=crio-e7ac08715fa184046b43a0138160284d16bc7f6cfc379a72c4210e225dcbf801.scope mems_allowed=0-1
# [856808.927995] CPU: 11 PID: 3223024 Comm: stress-ng-vm Kdump: loaded Tainted: G               ------------ T 3.10.0-1062.12.1.el7.x86_64 #1
# [856808.927997] Hardware name: Dell Inc. PowerEdge R720xd/068CDY, BIOS 2.5.2 01/28/2015
# [856808.927999] Call Trace:
# [856808.928009]  [<ffffffff9bb7ac43>] dump_stack+0x19/0x1b
# [856808.928014]  [<ffffffff9bb75d09>] dump_header+0x90/0x229
# [856808.928019]  [<ffffffff9b70a3db>] ? cred_has_capability+0x6b/0x120
# [856808.928023]  [<ffffffff9b4db4e2>] ? default_wake_function+0x12/0x20
# [856808.928028]  [<ffffffff9b5c1734>] oom_kill_process+0x254/0x3e0
# [856808.928033]  [<ffffffff9b63e776>] mem_cgroup_oom_synchronize+0x546/0x570
# [856808.928036]  [<ffffffff9b63dbf0>] ? mem_cgroup_charge_common+0xc0/0xc0
# [856808.928039]  [<ffffffff9b5c1fd4>] pagefault_out_of_memory+0x14/0x90
# [856808.928042]  [<ffffffff9bb74258>] mm_fault_error+0x6a/0x157
# [856808.928047]  [<ffffffff9bb888d1>] __do_page_fault+0x491/0x500
# [856808.928050]  [<ffffffff9bb88975>] do_page_fault+0x35/0x90
# [856808.928053]  [<ffffffff9bb84778>] page_fault+0x28/0x30
# [856808.928058] Task in /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podae2ab536_aca4_4113_91fe_58c7a25fff23.slice/crio-e7ac08715fa184046b43a0138160284d16bc7f6cfc379a72c4210e225dcbf801.scope killed as a result of limit of /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podae2ab536_aca4_4113_91fe_58c7a25fff23.slice
# [856808.928062] memory: usage 104857600kB, limit 104857600kB, failcnt 12
# [856808.928063] memory+swap: usage 104857600kB, limit 9007199254740988kB, failcnt 0
# [856808.928077] kmem: usage 0kB, limit 9007199254740988kB, failcnt 0
# [856808.928078] Memory cgroup stats for /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podae2ab536_aca4_4113_91fe_58c7a25fff23.slice: cache:0KB rss:0KB rss_huge:0KB mapped_file:0KB swap:0KB inactive_anon:0KB active_anon:0KB inactive_file:0KB active_file:0KB unevictable:0KB
# [856808.928103] Memory cgroup stats for /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podae2ab536_aca4_4113_91fe_58c7a25fff23.slice/crio-conmon-30b46554c02ef792d52004880a31bc81d5ffa8d436a9d5a28679b75e252176f6.scope: cache:24KB rss:92KB rss_huge:0KB mapped_file:0KB swap:0KB inactive_anon:12KB active_anon:104KB inactive_file:0KB active_file:0KB unevictable:0KB
# [856808.928138] Memory cgroup stats for /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podae2ab536_aca4_4113_91fe_58c7a25fff23.slice/crio-30b46554c02ef792d52004880a31bc81d5ffa8d436a9d5a28679b75e252176f6.scope: cache:0KB rss:676KB rss_huge:0KB mapped_file:0KB swap:0KB inactive_anon:0KB active_anon:676KB inactive_file:0KB active_file:0KB unevictable:0KB
# [856808.928156] Memory cgroup stats for /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podae2ab536_aca4_4113_91fe_58c7a25fff23.slice/crio-conmon-e7ac08715fa184046b43a0138160284d16bc7f6cfc379a72c4210e225dcbf801.scope: cache:8KB rss:88KB rss_huge:0KB mapped_file:0KB swap:0KB inactive_anon:4KB active_anon:92KB inactive_file:0KB active_file:0KB unevictable:0KB
# [856808.928170] Memory cgroup stats for /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podae2ab536_aca4_4113_91fe_58c7a25fff23.slice/crio-e7ac08715fa184046b43a0138160284d16bc7f6cfc379a72c4210e225dcbf801.scope: cache:104856196KB rss:516KB rss_huge:0KB mapped_file:104856196KB swap:0KB inactive_anon:104856156KB active_anon:500KB inactive_file:0KB active_file:0KB unevictable:16KB
# [856808.928196] [ pid ]   uid  tgid total_vm      rss nr_ptes swapents oom_score_adj name
# [856808.928447] [3221522]     0 3221522    30941      267      29        0          -999 conmon
# [856808.928452] [3221550]     0 3221550    25554      246       7        0          -998 pod
# [856808.928458] [3221830]     0 3221830    30941      266      29        0          -999 conmon
# [856808.928462] [3221855]     0 3221855     2404      173      10        0           969 iperf3
# [856808.928466] [3223000]     0 3223000    13828      912      24        0          1000 stress-ng
# [856808.928469] [3223021]     0 3223021    13829      173      23        0          1000 stress-ng-vm
# [856808.928471] [3223024]     0 3223024 39335429 26213613   51222        0          1000 stress-ng-vm
# [856808.928479] Memory cgroup out of memory: Kill process 3223024 (stress-ng-vm) score 2001 or sacrifice child
# [856808.928524] Killed process 3223024 (stress-ng-vm), UID 0, total-vm:157341716kB, anon-rss:284kB, file-rss:340kB, shmem-rss:104853828kB




# restore
oc delete -n demo -f demo.yaml


```