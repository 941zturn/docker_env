# vGPU sharing

- 重点在 allocate，这个是 kubelet请求device plugin，给出了一个device list，让device plugin给出命令参数，然后kubelet去启动。
- 那么重点，就在kubelet怎么算出来这个device list的，我们就改这部分就好了，似乎是scheduler
- 可以直接试试aliyun， 先看看scheduler能不能用。
  - 不行，ocp的scheduler扩展给封死了。
  - 定制openshift的sheduler
    - 不用定制了，openshift支持直接配置scheduler extender
    - 我们来看看aliyun的scheduler extender里面的逻辑。
      - GetUpdatedPodAnnotationSpec 在这个函数里面，改了annotation，太好了，我们就用这个改
      - 现在的问题是，aliyun定制的device plugin干什么的，好像不是分配，分配在extender里面做完了。
- 看device plugin 似乎可以直接装，那试试看
  - 要用helm的方式，改一下nvidia 官方的helm
  - 不用helm，直接用operator就可以，因为helm也是启动operator
    - aliyun的device plugin需要一个环境变量，NODE_NAME，operator方式不能动态注入，没办法，先写一个静态的 worker-0 吧，反正就一块gpu
    - device plugin运行起来的，但是报错，没有权限，role, 和 role-binding 创建了也不行，要再看看。

- 简单来说，最终控制pod能访问那个GPU的能力，由英伟达官方driver中的oci/crio hook提供，它将检查环境变量NVIDIA_VISIBLE_DEVICES，并根据这个环境变量，设置pod能够访问的GPU.
  - 完蛋，不能改env，只能改annotation
  - 只能定制nvidia toolkit




https://www.openshift.com/blog/extending-the-runtime-functionality

nvidia driver
- driver daemon编译了源代码，并且modprob nvidia加载了驱动
  - 这里面有一个点，是否在container里面modprob，在宿主机上这个module也加载了，或者说，对于modprob，是没有container和宿主机的区别的。
- driver daemon把文件系统，挂载到了主机的/run/nvidia/driver下面
- toolkit镜像，做了一个crio-hooks, 配置文件在/etc/containers/oci/hooks.d/
  - 我们预计，这个hooks回读取container的环境变量，来加载/dev/nvidia0到容器中，但是没找到。
  - https://gitlab.com/nvidia/container-toolkit/container-runtime 这里应该有所有hook支持的参数
  - 这样看，有一个思路，是最后在container上面，改NVIDIA_VISIBLE_DEVICES这个环境变量，控制container里面能看到的显卡。

```bash
oc image mirror \
registry.cn-hangzhou.aliyuncs.com/acs/k8s-gpushare-schd-extender:1.11-d170d8a  \
quay.io/wangzheng422/qimgs:k8s-gpushare-schd-extender-1.11-d170d8a

oc image mirror \
registry.cn-hangzhou.aliyuncs.com/acs/k8s-gpushare-plugin:v2-1.11-aff8a23 \
quay.io/wangzheng422/qimgs:k8s-gpushare-plugin.v2-1.11-aff8a23-rhcos4.6

oc image mirror \
nvcr.io/nvidia/k8s/container-toolkit@sha256:81295a9eca36cbe5d94b80732210b8dc7276c6ef08d5a60d12e50479b9e542cd \
quay.io/wangzheng422/qimgs:nvidia.container-toolkit.ocp.4.6

curl -O https://raw.githubusercontent.com/AliyunContainerService/gpushare-scheduler-extender/master/config/gpushare-schd-extender.yaml
# replace docker image
cd /data/install
sed -i 's/image:.*/image: quay.io\/wangzheng422\/qimgs:gpushare-scheduler-extender-2021-02-26-1339/' gpushare-schd-extender.yaml
oc delete -f gpushare-schd-extender.yaml
oc create -f gpushare-schd-extender.yaml


cd /data/install
cat << EOF > ./policy.cfg
    {
    "kind" : "Policy",
    "apiVersion" : "v1",
    "predicates" : [
            {"name" : "MaxGCEPDVolumeCount"},
            {"name" : "GeneralPredicates"},
            {"name" : "MaxAzureDiskVolumeCount"},
            {"name" : "MaxCSIVolumeCountPred"},
            {"name" : "CheckVolumeBinding"},
            {"name" : "MaxEBSVolumeCount"},
            {"name" : "MatchInterPodAffinity"},
            {"name" : "CheckNodeUnschedulable"},
            {"name" : "NoDiskConflict"},
            {"name" : "NoVolumeZoneConflict"},
            {"name" : "PodToleratesNodeTaints"}
            ],
    "priorities" : [
            {"name" : "LeastRequestedPriority", "weight" : 1},
            {"name" : "BalancedResourceAllocation", "weight" : 1},
            {"name" : "ServiceSpreadingPriority", "weight" : 1},
            {"name" : "NodePreferAvoidPodsPriority", "weight" : 1},
            {"name" : "NodeAffinityPriority", "weight" : 1},
            {"name" : "TaintTolerationPriority", "weight" : 1},
            {"name" : "ImageLocalityPriority", "weight" : 1},
            {"name" : "SelectorSpreadPriority", "weight" : 1},
            {"name" : "InterPodAffinityPriority", "weight" : 1},
            {"name" : "EqualPriority", "weight" : 1}
            ],
    "extenders": [
            {
              "urlPrefix": "http://127.0.0.1:32766/gpushare-scheduler",
              "filterVerb": "filter",
              "bindVerb":   "bind",
              "enableHttps": false,
              "nodeCacheCapable": true,
              "managedResources": [
                {
                  "name": "aliyun.com/gpu-mem",
                  "ignoredByScheduler": false
                }
              ],
              "ignorable": false
            }
          ]
    }
   
EOF
oc delete configmap -n openshift-config  scheduler-policy
oc create configmap -n openshift-config --from-file=policy.cfg scheduler-policy


oc patch Scheduler cluster --type='merge' -p '{"spec":{"policy":{"name":"scheduler-policy"}}}' --type=merge



```
## ocp base image download

我们需要扩展 scheduler extender，就需要从源代码级别重新编译openshift相关组件，想编译，就要下载基础镜像，而下载基础镜像，需要账号密码。现在已知有2种方法，一个是向openshift-sme@redhat.com申请github账号权限，另外一个是在redhat公司内网下载，以下分别说

### openshift-sme

参照这个文档，申请自己的github账号，能加入 openshift organization。

https://source.redhat.com/groups/public/atomicopenshift/atomicopenshift_wiki/openshift_onboarding_checklist_for_github

如果批准了，就做下面的事情：

Step 1: 访问openshift cluster： https://console-openshift-console.apps.ci.l2s4.p1.openshiftapps.com
Step 2： 用您的github账号登陆。
Step3： 拷贝登陆openshift cluster的命令， 类似: oc login ....
Step4: 登陆openshift cluster成功后，登陆registry： docker login registry.ci.openshift.org -u <github-id> -p $(oc whoami -t)
Step5: docker pull 测试一下能否pull下来base image.

```bash
podman pull registry.ci.openshift.org/ocp/4.6:base
podman pull registry.ci.openshift.org/ocp/builder:rhel-8-golang-1.15-openshift-4.6

```

### registry-proxy.engineering.redhat.com

另外的这种方案，不需要账号，但是需要在红帽的内网做。

首先去下载 https://password.corp.redhat.com/RH-IT-Root-CA.crt 

然后把这个 RH-IT-Root-CA.crt 导入系统
```bash
/bin/cp -f RH-IT-Root-CA.crt  /etc/pki/ca-trust/source/anchors/
update-ca-trust extract

```

然后下载镜像
```bash
podman pull registry-proxy.engineering.redhat.com/rh-osbs/openshift-ose-base:v4.6.0

```

## cluster-kube-scheduler-operator

我们试图定制scheduler来满足阿里或者腾讯的gpu sharing对scheduler extender的需求

关键看  /etc/kubernetes/manifests/kube-scheduler-pod.yaml 是怎么创建的，因为kube-scheduler是一个static pod，他是靠上面的文件，通过kubelet来创建的。

改 pkg/operator/targetconfigcontroller/targetconfigcontroller.go 这里的代码，应该就可以了。




```bash


```



## others
首先，去 https://post-office.corp.redhat.com/mailman/listinfo/openshift-sme 申请加入这个邮件列表。

然后，向 openshift-sme@redhat.com 发邮件，申请自己的github账号，能加入 openshift organization。

```bash
oc image mirror docker.io/wangzheng422/centos:centos7-test \
quay.io/wangzheng422/qimgs:centos7-test

oc image mirror nvcr.io/nvidia/k8s/cuda-sample:vectoradd-cuda10.2-ubi8 \
quay.io/wangzheng422/qimgs:cuda-sample.vectoradd-cuda10.2-ubi8

oc image mirror docker.io/wangzheng422/imgs:tensorrt-ljj-2021-01-21-1151 \
quay.io/wangzheng422/qimgs:tensorrt-ljj-2021-01-21-1151


```

```yaml

apiVersion: nvidia.com/v1
kind: ClusterPolicy
metadata:
  name: gpu-cluster-policy
spec:
  dcgmExporter:
    nodeSelector: {}
    imagePullSecrets: []
    resources: {}
    affinity: {}
    podSecurityContext: {}
    repository: nvcr.io/nvidia/k8s
    securityContext: {}
    version: 'sha256:85016e39f73749ef9769a083ceb849cae80c31c5a7f22485b3ba4aa590ec7b88'
    image: dcgm-exporter
    tolerations: []
  devicePlugin:
    nodeSelector: {}
    imagePullSecrets: []
    resources: {}
    affinity: {}
    podSecurityContext: {}
    repository: quay.io/wangzheng422
    securityContext: {}
    version: gpu-aliyun-device-plugin-2021-02-24-1346
    image: qimgs
    tolerations: []
    args:
      - 'gpushare-device-plugin-v2'
      - '-logtostderr'
      - '--v=5'
    env:
      - name: NODE_NAME
        value: worker-0
  driver:
    nodeSelector: {}
    imagePullSecrets: []
    resources: {}
    affinity: {}
    podSecurityContext: {}
    repository: nvcr.io/nvidia
    securityContext: {}
    repoConfig:
      configMapName: repo-config
      destinationDir: /etc/yum.repos.d
    version: 'sha256:324e9dc265dec320207206aa94226b0c8735fd93ce19b36a415478c95826d934'
    image: driver
    tolerations: []
  gfd:
    nodeSelector: {}
    imagePullSecrets: []
    resources: {}
    affinity: {}
    podSecurityContext: {}
    repository: nvcr.io/nvidia
    securityContext: {}
    version: 'sha256:8d068b7b2e3c0b00061bbff07f4207bd49be7d5bfbff51fdf247bc91e3f27a14'
    image: gpu-feature-discovery
    tolerations: []
    migStrategy: single
    sleepInterval: 60s
  operator:
    defaultRuntime: crio
    validator:
      image: cuda-sample
      imagePullSecrets: []
      repository: nvcr.io/nvidia/k8s
      version: 'sha256:2a30fe7e23067bc2c3f8f62a6867702a016af2b80b9f6ce861f3fea4dfd85bc2'
    deployGFD: true
  toolkit:
    nodeSelector: {}
    imagePullSecrets: []
    resources: {}
    affinity: {}
    podSecurityContext: {}
    repository: nvcr.io/nvidia/k8s
    securityContext: {}
    version: 'sha256:81295a9eca36cbe5d94b80732210b8dc7276c6ef08d5a60d12e50479b9e542cd'
    image: container-toolkit
    tolerations: []

```

```
I0224 14:03:18.327365       1 main.go:18] Start gpushare device plugin
I0224 14:03:18.327568       1 gpumanager.go:28] Loading NVML
I0224 14:03:18.330610       1 gpumanager.go:37] Fetching devices.
I0224 14:03:18.330773       1 gpumanager.go:43] Starting FS watcher.
I0224 14:03:18.331006       1 gpumanager.go:51] Starting OS watcher.
I0224 14:03:18.339228       1 nvidia.go:64] Deivce GPU-368a683d-e1e5-5928-7ac3-c19e475b3839's Path is /dev/nvidia0
I0224 14:03:18.339377       1 nvidia.go:69] # device Memory: 16127
I0224 14:03:18.339398       1 nvidia.go:40] set gpu memory: 15
I0224 14:03:18.339410       1 nvidia.go:76] # Add first device ID: GPU-368a683d-e1e5-5928-7ac3-c19e475b3839-_-0
I0224 14:03:18.339437       1 nvidia.go:79] # Add last device ID: GPU-368a683d-e1e5-5928-7ac3-c19e475b3839-_-14
I0224 14:03:18.339450       1 server.go:43] Device Map: map[GPU-368a683d-e1e5-5928-7ac3-c19e475b3839:0]
I0224 14:03:18.339494       1 server.go:44] Device List: [GPU-368a683d-e1e5-5928-7ac3-c19e475b3839]
W0224 14:03:18.355711       1 gpumanager.go:66] Failed to get device plugin due to nodes "worker-0" is forbidden: User "system:serviceaccount:gpu-operator-resources:nvidia-device-plugin" cannot get resource "nodes" in API group "" at the cluster scope
```

```bash
cat << EOF > /data/ocp4/ali-gpu-rbac.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: gpushare-device-plugin
rules:
  - verbs:
      - get
      - list
      - watch
    apiGroups:
      - ''
    resources:
      - nodes
  - verbs:
      - create
      - patch
    apiGroups:
      - ''
    resources:
      - events
  - verbs:
      - update
      - patch
      - get
      - list
      - watch
    apiGroups:
      - ''
    resources:
      - pods
  - verbs:
      - patch
      - update
    apiGroups:
      - ''
    resources:
      - nodes/status
  - verbs:
      - use
    apiGroups:
      - security.openshift.io
    resources:
      - securitycontextconstraints
    resourceNames:
      - privileged
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: gpushare-device-plugin-nvidia
subjects:
  - kind: ServiceAccount
    name: nvidia-device-plugin
    namespace: gpu-operator-resources
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: gpushare-device-plugin

EOF
oc apply -f /data/ocp4/ali-gpu-rbac.yaml

oc describe clusterroles gpushare-device-plugin-nvidia

#
rm -rf /data/gpu
mkdir -p /data/gpu
cd /data/gpu
git clone https://github.com/wangzheng422/gpushare-device-plugin.git
cd gpushare-device-plugin
git checkout wzh-dev

var_date=$(date '+%Y-%m-%d-%H%M')
echo $var_date
buildah bud --format=docker -t quay.io/wangzheng422/qimgs:gpu-aliyun-device-plugin-$var_date -f Dockerfile .

# quay.io/wangzheng422/qimgs:gpu-aliyun-device-plugin-2021-02-24-1346
buildah push quay.io/wangzheng422/qimgs:gpu-aliyun-device-plugin-$var_date

#
rm -rf /data/gpu
mkdir -p /data/gpu
cd /data/gpu
git clone https://github.com/wangzheng422/gpushare-scheduler-extender.git
cd gpushare-scheduler-extender
git checkout wzh-dev

var_date=$(date '+%Y-%m-%d-%H%M')
echo $var_date
buildah bud --format=docker -t quay.io/wangzheng422/qimgs:gpushare-scheduler-extender-$var_date -f Dockerfile .

# quay.io/wangzheng422/qimgs:gpushare-scheduler-extender-2021-02-26-1339
buildah push quay.io/wangzheng422/qimgs:gpushare-scheduler-extender-$var_date


# test it
oc image mirror docker.io/cheyang/gpu-player:v2  quay.io/wangzheng422/qimgs:gpu-player.v2

cat << EOF > /data/ocp4/gpu.test.yaml
---
kind: Deployment
apiVersion: apps/v1
metadata:
  annotations:
  name: demo1
  labels:
    app: demo1
spec:
  replicas: 1
  selector:
    matchLabels:
      app: demo1
  template:
    metadata:
      labels:
        app: demo1
    spec:
      # nodeSelector:
      #   kubernetes.io/hostname: 'worker-0'
      restartPolicy: Always
      containers:
        - name: demo1
          image: "docker.io/wangzheng422/imgs:tensorrt-ljj-2021-01-21-1151"
          env:
            - name: NVIDIA_VISIBLE_DEVICES
              valueFrom:
                fieldRef:
                  fieldPath: metadata.annotations['ALIYUN_COM_GPU_MEM_IDX']
          resources:
            limits:
              # GiB
              aliyun.com/gpu-mem: 3

EOF
oc create -n demo -f /data/ocp4/gpu.test.yaml

# nvidia gpu-operator, self-dev, based on 1.5.2
yum install -y golang
rm -rf /data/gpu
mkdir -p /data/gpu
cd /data/gpu
git clone https://github.com/wangzheng422/gpu-operator.git
cd gpu-operator
git checkout wzh-dev

export GOPROXY="https://proxy.golang.org,direct"
make build

var_date=$(date '+%Y-%m-%d-%H%M')
echo $var_date
export VERSION="wzh-1.6.0"
export IMAGE="quay.io/wangzheng422/qimgs"
export TAG="nvidia-gpu-operator.wzh-1.6.0-$var_date"
export DOCKER="podman"
make prod-image

# quay.io/wangzheng422/qimgs:nvidia-gpu-operator.wzh-1.6.0-2021-02-28-0323
podman push quay.io/wangzheng422/qimgs:nvidia-gpu-operator.wzh-1.6.0-$var_date

#
# get nvidia gpu operator from operator hub
RELEASE_VERSION=v1.4.2
curl -OJL https://github.com/operator-framework/operator-sdk/releases/download/${RELEASE_VERSION}/operator-sdk_linux_amd64
chmod +x operator-sdk_linux_amd64
sudo cp operator-sdk_linux_amd64 /usr/local/bin/operator-sdk
rm -f operator-sdk_linux_amd64
operator-sdk version

cd /data/gpu/gpu-operator/
var_date=$(date '+%Y-%m-%d-%H%M')
echo $var_date

operator-sdk bundle validate bundle/

podman build -t quay.io/wangzheng422/qimgs:nvidia-gpu-operator.wzh-bundle-1.5.2-$var_date -f bundle.Dockerfile .

# quay.io/wangzheng422/qimgs:nvidia-gpu-operator.wzh-bundle-1.5.2-2021-02-28-0537
podman push quay.io/wangzheng422/qimgs:nvidia-gpu-operator.wzh-bundle-1.5.2-$var_date

operator-sdk bundle validate quay.io/wangzheng422/qimgs:nvidia-gpu-operator.wzh-bundle-1.5.2-$var_date

#
mkdir -p /data/tmp
cd /data/tmp
wget https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/ocp/latest-4.6/opm-linux.tar.gz
tar xvf opm-linux.tar.gz
sudo mv ./opm /usr/local/bin/
opm version

var_date=$(date '+%Y-%m-%d-%H%M')
echo $var_date

cd /data/gpu/gpu-operator/
opm index add --bundles quay.io/wangzheng422/qimgs:nvidia-gpu-operator.wzh-bundle-1.5.2-2021-02-28-0537 \
--tag quay.io/wangzheng422/qimgs:registry-wzh-index.$var_date

# quay.io/wangzheng422/qimgs:registry-wzh-index.2021-02-28-1338
podman push quay.io/wangzheng422/qimgs:registry-wzh-index.$var_date

#



```



## others
```yaml
apiVersion: nvidia.com/v1
kind: ClusterPolicy
metadata:
  name: gpu-cluster-policy
spec:
  dcgmExporter:
    nodeSelector: {}
    imagePullSecrets: []
    resources: {}
    affinity: {}
    podSecurityContext: {}
    repository: nvcr.io/nvidia/k8s
    securityContext: {}
    version: 'sha256:85016e39f73749ef9769a083ceb849cae80c31c5a7f22485b3ba4aa590ec7b88'
    image: dcgm-exporter
    tolerations: []
  devicePlugin:
    nodeSelector: {}
    imagePullSecrets: []
    resources: {}
    affinity: {}
    podSecurityContext: {}
    repository: nvcr.io/nvidia
    securityContext: {}
    version: 'sha256:f7bf5955a689fee4c1c74dc7928220862627adc97e00a4b585f9c31965e79625'
    image: k8s-device-plugin
    tolerations: []
    args:
      - '--mig-strategy=single'
      - '--pass-device-specs=false'
      - '--fail-on-init-error=true'
      - '--device-list-strategy=envvar'
      - '--nvidia-driver-root=/run/nvidia/driver'
  driver:
    nodeSelector: {}
    imagePullSecrets: []
    resources: {}
    affinity: {}
    podSecurityContext: {}
    repository: nvcr.io/nvidia
    securityContext: {}
    repoConfig:
      configMapName: repo-config
      destinationDir: /etc/yum.repos.d
    version: 'sha256:324e9dc265dec320207206aa94226b0c8735fd93ce19b36a415478c95826d934'
    image: driver
    tolerations: []
  gfd:
    nodeSelector: {}
    imagePullSecrets: []
    resources: {}
    affinity: {}
    podSecurityContext: {}
    repository: nvcr.io/nvidia
    securityContext: {}
    version: 'sha256:8d068b7b2e3c0b00061bbff07f4207bd49be7d5bfbff51fdf247bc91e3f27a14'
    image: gpu-feature-discovery
    sleepInterval: 60s
    tolerations: []
    migStrategy: single
  operator:
    defaultRuntime: crio
    deployGFD: true
    validator:
      image: cuda-sample
      imagePullSecrets: []
      repository: nvcr.io/nvidia/k8s
      version: 'sha256:2a30fe7e23067bc2c3f8f62a6867702a016af2b80b9f6ce861f3fea4dfd85bc2'
  toolkit:
    nodeSelector: {}
    imagePullSecrets: []
    resources: {}
    affinity: {}
    podSecurityContext: {}
    repository: nvcr.io/nvidia/k8s
    securityContext: {}
    version: 'sha256:81295a9eca36cbe5d94b80732210b8dc7276c6ef08d5a60d12e50479b9e542cd'
    image: container-toolkit
    tolerations: []



```