# RH.442

![](imgs/2021-01-17-19-20-07.png)
```bash
ps aux
ps -p $(pidof sshd)
ps ax --format pid,%mem,cmd --sort -%mem

free -m
lsblk -fp
findmnt -S /dev/vda1

pidstat -p 23354 1 3

```
![](imgs/2021-01-17-21-07-13.png)
```bash
# sysstat-collect.service
yum install sysstat
cp /usr/lib/systemd/system/sysstat-collect.timer \
/etc/systemd/system/sysstat-collect.timer
systemctl enable --now sysstat-collect.timer

yum install pcp-system-tools
pcp free
pcp dstat

pmchart

pmlogger /var/log/pcp/pmlogger/host.example.com
```
![](imgs/2021-01-17-21-13-20.png)
```bash
lscpu -p
# # The following is the parse-able format, which can be fed to other
# # programs. Each different item in every column has an unique ID
# # starting from zero.
# # CPU,Core,Socket,Node,,L1d,L1i,L2
# 0,0,0,0,,0,0,0
# 1,1,0,0,,1,1,1
# 2,2,0,0,,2,2,1
# 3,3,0,0,,3,3,0

getconf -a
# PAGESIZE                           4096
# PAGE_SIZE                          4096
# ...output omitted...
# _AVPHYS_PAGES                      36380772
# _NPROCESSORS_CONF                  24
# _NPROCESSORS_ONLN                  24
# _PHYS_PAGES                        37209409
# ...output omitted...
# LEVEL1_ICACHE_SIZE                 32768
# LEVEL1_ICACHE_ASSOC                4
# LEVEL1_ICACHE_LINESIZE             64
# LEVEL1_DCACHE_SIZE                 32768
# LEVEL1_DCACHE_ASSOC                8
# LEVEL1_DCACHE_LINESIZE             64
# LEVEL2_CACHE_SIZE                  262144
# LEVEL2_CACHE_ASSOC                 8
# LEVEL2_CACHE_LINESIZE              64
# LEVEL3_CACHE_SIZE                  12582912
# LEVEL3_CACHE_ASSOC                 16
# LEVEL3_CACHE_LINESIZE              64
# LEVEL4_CACHE_SIZE                  0
# LEVEL4_CACHE_ASSOC                 0
# LEVEL4_CACHE_LINESIZE              0

# Retrieving SMBIOS/DMI information
dmidecode
ls /sys/class/dmi/id/
head /sys/class/dmi/id/product_*

lspci
lspci -vv

lsusb
lsusb -vv

lstopo-no-graphics

lshw -short
lshw -C system

# Reporting Hardware Errors
yum install rasdaemon
systemctl enable --now rasdaemon
ras-mc-ctl --help
ras-mc-ctl --summary
ras-mc-ctl --errors

virsh dumpxml guest |grep "q35"
# <type arch='x86_64' machine='pc-q35-rhel7.6.0'>hvm</type>

virsh dumpxmlguest |grep "cpu mode"
# <cpu mode='host-passthrough'>

lspci | grep balloon
# 07:00.0 Unclassified device [00ff]: Red Hat, Inc. Virtio memory balloon (rev 01)

grep balloon guest.xml
# <memballoon model='virtio'>
#   <alias name='balloon0'/>
# </memballoon>

kvm_stat
perf kvm stat -- sleep 10

perf kvm --guest --guestmodules=guest-modules \
--guestkallsyms=guest-kallsyms record -a
perf kvm --guest --guestmodules=guest-modules \
--guestkallsyms=guest-kallsyms report --force > guest-analyze

ls -l /proc/sys/kernel/{osrelease,threads-max}
cat /proc/sys/kernel/osrelease
# 4.18.0-80.el8.x86_64
cat /proc/sys/kernel/threads-max
# 14257

```
![](imgs/2021-01-17-21-25-47.png)
```bash
modinfo loop
modinfo -p loop

tuned-adm active
tuned-adm list
tuned-adm profile throughput-performance
tuned-adm active
tuned-adm recommend

systemd-cgtop
systemd-cgls

systemd-run --slice=example.slice sleep 10d
systemd-cgls /example.slice/run-4321.service
# /example.slice/run-4321.service:
# └─4322 /bin/sleep 10d
systemctl set-property example.service MemoryAccounting=yes
systemctl set-property example.service MemoryLimit=2048M

```
Control systemd slices and cgroups these three ways:

Enable CPU, memory, and/or block I/O accounting for a service or a slice.

Place resource limits on an individual service or slice.

Run services in a custom slice.

```bash
[Unit]
Description=Example Custom Slice
[Slice]
CPUShares=2048
MemoryAccounting=true


```
![](imgs/2021-01-19-10-25-22.png)

![](imgs/2021-01-19-10-25-30.png)

```bash
yum install perf
# Debug symbols are required for performance data 
subscription-manager repos --enable \
rhel-8-for-x86_64-baseos-debug-rpms
uname -r
# 4.18.0-80.el8.x86_64
yum install kernel-debuginfo-4.18.0-80.el8.x86_64 \
kernel-debuginfo-common-x86_64-4.18.0-80.el8.x86_64

perf list

perf stat dd if=/dev/zero of=/dev/null bs=2048 count=10000000

perf stat \
-e cycles,instructions,cache-references,cache-misses,bus-cycles \
-a sleep 10

perf record \
-e cpu-clock,instructions,cache-misses,context-switches \
dd if=/dev/zero of=/dev/null bs=2048 count=10000000

perf report --stdio

perf record -o cs-syswide.data -e context-switches -a sleep 10

perf report -i cs-syswide.data --stdio

strace -c uname
# % time     seconds  usecs/call     calls    errors syscall
# ------ ----------- ----------- --------- --------- ----------------
#  82.28    0.000130          43         3           open
#  17.72    0.000028          14         2           munmap
#   0.00    0.000000           0         1           read

# SystemTap Host System
subscription-manager repos --enable \
rhel-8-for-x86_64-baseos-debug-rpms
yum install systemtap
stap-prep

stap -v /usr/share/systemtap/examples/process/syscalls_by_proc.stp

# create the syscalls_by_proc module in the current directory.
stap -p 4 -v -m syscalls_by_proc \
/usr/share/systemtap/examples/process/syscalls_by_proc.stp
mkdir /lib/modules/$(uname -r)/systemtap
cp /root/syscalls_by_proc.ko /lib/modules/$(uname -r)/systemtap

staprun syscalls_by_proc

yum install systemtap-runtime
firefox /usr/share/systemtap/examples/index.html


```
Extended Berkeley Packet Filter (eBPF) is an in-kernel virtual machine that runs its own bytecode in a secured, restricted environment. 

eBPF works by attaching programs to various kernel points, such as kprobes, tracepoints, and perf events. There are many performance analysis tools that leverage the extended kernel functionality, allowing for more than just filtering packets. These enhancements allow custom analysis programs to be executed on Linux for dynamic tracing, static tracing, and profiling events.

![](imgs/2021-01-23-14-05-48.png)

```bash
yum install -y bcc-tools

ls /usr/share/bcc/tools

ls /usr/share/bcc/tools/doc

```

![](imgs/2021-01-23-14-49-00.png)

![](imgs/2021-01-23-14-57-58.png)

Kernel Tunables for Scheduling Policies

The sysctl command can tune CFS scheduler tunables. These tunables are located in the /proc/sys/kernel/ directory Some are listed here:

sched_latency_ns
Defines the targeted preemption latency. Increasing this variable value increases the timeslice of a CPU bound process. The targeted latency is the period in which all run queue tasks are scheduled at least once. The duration is defined in nanoseconds (ns).

sched_min_granularity_ns
Defines the minimum amount of time any scheduled process must be allowed to run before being preempted by another process. The duration is defined in nanoseconds (ns). This variable value must be smaller than sched_latency_ns. If the number of running tasks in the queue is greater than sched_latency_ns divided by sched_min_granularity_ns, then there are too many tasks on the system, and the sched_latency_ns variable cannot satisfy all the minimum granularity time requirements for a task. To accommodate such cases, the scheduler increases the scheduling period by multiplying the number of running of tasks by the sched_min_granularity_ns value.

sched_migration_cost_ns
Defines the require elapsed time after a process' last execution to be considered for migration to another CPU. The duration is defined in nanoseconds (ns). Increasing this variable value reduces process migration across CPUs.

sched_rt_period_us
Defines the timeslice that provides runtime to CPU bound processes. The duration is defined in microseconds (µs). The default value of is equivalent to 100% CPU bandwidth.

sched_rt_runtime_us
Defines maximum CPU time that can be used by all real-time tasks. The duration is defined in microseconds (µs). The scheduler reserves the difference between the sched_rt_period_us and sched_rt_runtime_us variable value for processes using non-real-time scheduling policies. The default value for sched_rt_period_us is 1000000 µs (1 second) and for sched_rt_runtime_us is 950000 µs (0.95 second). This provides 0.05 second to be consumed by processes using non-real-time policies. These default values prevent a real-time process from locking up the system. This safeguard mechanism is known as the real-time scheduler throttling.

sched_rr_timeslice_us
Defines the timeslice that a SCHED_RR process is allowed to run before the process is preempted and put at the end of the CPU bound process queue. The duration is defined in microseconds (µs).

```bash
chrt -d --sched-runtime 5000000 --sched-deadline 10000000 \
 --sched-period 16666666 0 simple_task

tuna --show_threads

cat /etc/systemd/system/sshd.service.d/10-scheduler.conf
# [Service]
# CPUSchedulingPolicy=rr
# CPUSchedulingPriority=10

# Viewing Process Scheduler Statistics
cat /proc/sched_debug

cat /proc/sched_debug

cat /proc/91/sched

ls /sys/fs/cgroup/cpuset
cat /proc/1088/cpuset
cat /sys/fs/cgroup/cpuset/cpuset.cpus
cat /sys/fs/cgroup/cpuset/cpuset.mems

printf '%032x' $((2**0 + 2**1 + 2**3 + 2**5 + 2**6 + 2**7 + 2**8))

egrep "Cpus*|Mems*" /proc/12520?/status

lstopo --no-legend --no-io 

lscpu

lshw -class memory

```
![](imgs/2021-01-23-17-39-41.png)

![](imgs/2021-01-23-17-39-50.png)

On x86 platforms, each page of memory contains control bits that can be used to disable caching of pages, and to disable write-back caching. The Linux kernel clears both bits on all memory pages, so all memory accesses are cached with write-back caching.

![](imgs/2021-01-23-17-41-32.png)

![](imgs/2021-01-23-17-42-15.png)

Direct mapped cache
Direct mapped cache is the least expensive type of cache memory. Each line of direct mapped cache can only cache a specific location in main memory. With direct mapped cache, the search time is faster because cache data maps to one specific memory location, but the cache hit rate is low.

Fully associative cache
Fully associative cache memory is the most flexible type of cache memory, and consequently the most expensive because it requires the most circuitry to implement. A fully associative cache memory line can cache any location in main memory. With fully associative cache, the search time is extremely long because the CPU has to look through the entire cache to find out if the data is present before searching main memory, but the cache hit rate is high.

Set associative cache
Set associative cache memory provides a good compromise between direct mapped cache and fully associative cache. This cache memory is usually referred to as n-way set associative, where n is some power of 2. Set associative cache memory allows a memory location to be cached into any one of n lines of cache. Most systems compromise and use set associative cache memory.

The valgrind utility collection provides a set of tools for memory debugging and profiling. One of the tools provided in the valgrind toolset is the cachegrind tool. 

```bash
valgrind --tool=cachegrind cache1

perf list

perf stat -e instructions,cycles, \
L1-dcache-loads,L1-dcache-load-misses,LLC-load-misses,LLC-loads \
tar cvf backup.tar /etc

perf stat -e instructions,cycles,L1-dcache-loads,L1-dcache-load-misses,LLC-load-misses,LLC-loads \
-p 6684


```
Computer systems organize memory into fixed-size chunks called pages. The size of a page depends on the processor architecture; for x86_64, the standard page size is 4 KiB. The physical RAM on a system is divided into page frames; one page frame holds one page of data.

The size of a process's virtual address space does not depend on the installed physical RAM but rather depends on the processor architecture. On a 64-bit x86-64 system, the address space is 264 bytes (16 EiB) in size.

![](imgs/2021-01-23-19-48-05.png)

The latest generation of processors reserves the highest 7 bits of the virtual address, which causes a large block of virtual addresses to be unavailable for use by processes. Only 57 bits are left. With 57 bits, the kernel can address 128 PiB (257) of RAM. 

Older processors reserve the highest 16 bits of the virtual address; the system can only use 48 bits for the virtual addresses. The addressable virtual memory is 256 TiB.

```bash
cat /proc/cpuinfo | grep address
# address sizes   : 46 bits physical, 48 bits virtual

systemctl cat sshd.service
# [Service]
# MemoryLimit=1073741824


```
When a process accesses a page that the system has not yet mapped to a physical page frame in the table, the kernel generates a page fault event.

The event occurs when the process accesses a page for the first time after allocation. In that situation, the kernel generates a minor page fault and allocates a new physical page frame from RAM. A minor page fault causes a small amount of overhead.

However, if the physical page frame is missing and the system needs to retrieve that page of memory from disk, the kernel generates a major page fault event.

```bash
# Monitoring Page Faults
ps -o pid,minflt,majflt,comm -C qemu-kvm

perf stat -e minor-faults,major-faults -p 6684

```

![](imgs/2021-01-23-20-07-36.png)




